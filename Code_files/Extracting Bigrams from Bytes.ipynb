{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Metrices\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\nikhi\\\\606 Capstone'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\UMBC\\\\606 Capstone\\\\Data\\\\train'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"C:/UMBC/606 Capstone/Data/train\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not os.path.isdir('bytesFile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-grams from 1-grams\n",
    "\n",
    "grams = \"ID,00,01,02,03,04,05,06,07,08,09,0a,0b,0c,0d,0e,0f,10,11,12,13,14,15,16,17,18,19,1a,1b,1c,1d,1e,1f,20,21,22,23,24,25,26,27,28,29,2a,2b,2c,2d,2e,2f,30,31,32,33,34,35,36,37,38,39,3a,3b,3c,3d,3e,3f,40,41,42,43,44,45,46,47,48,49,4a,4b,4c,4d,4e,4f,50,51,52,53,54,55,56,57,58,59,5a,5b,5c,5d,5e,5f,60,61,62,63,64,65,66,67,68,69,6a,6b,6c,6d,6e,6f,70,71,72,73,74,75,76,77,78,79,7a,7b,7c,7d,7e,7f,80,81,82,83,84,85,86,87,88,89,8a,8b,8c,8d,8e,8f,90,91,92,93,94,95,96,97,98,99,9a,9b,9c,9d,9e,9f,a0,a1,a2,a3,a4,a5,a6,a7,a8,a9,aa,ab,ac,ad,ae,af,b0,b1,b2,b3,b4,b5,b6,b7,b8,b9,ba,bb,bc,bd,be,bf,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,ca,cb,cc,cd,ce,cf,d0,d1,d2,d3,d4,d5,d6,d7,d8,d9,da,db,dc,dd,de,df,e0,e1,e2,e3,e4,e5,e6,e7,e8,e9,ea,eb,ec,ed,ee,ef,f0,f1,f2,f3,f4,f5,f6,f7,f8,f9,fa,fb,fc,fd,fe,ff\"\n",
    "onegram = grams.split(',')\n",
    "\n",
    "s = ''\n",
    "for i in range(1,257):\n",
    "    for j in range(1,257):\n",
    "        s = s + onegram[i] + onegram[j] + ','\n",
    "\n",
    "s = s+'????'\n",
    "\n",
    "bi_grams = s.split(',')\n",
    "len(bi_grams)\n",
    "\n",
    "# Bi-grams string\n",
    "bi_gram = 'ID,'+s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_byte = \"C:/UMBC/606 Capstone/Data/train/byteFiles\"\n",
    "files = os.listdir(root_byte)\n",
    "filenames2=[]\n",
    "\n",
    "# Feature matrix to store bi-grams byte features\n",
    "feature_matrix = np.zeros((len(files),65537),dtype=int)\n",
    "k=0\n",
    "\n",
    "\n",
    "#program to convert into bag of words of bytefiles\n",
    "#this is custom-built bag of words this is unigram bag of words\n",
    "byte_feature_file=open('bigrams.csv','w+')\n",
    "byte_feature_file.write(bi_gram)\n",
    "byte_feature_file.write(\"\\n\")\n",
    "for file in tqdm(files):\n",
    "    filenames2.append(file)\n",
    "    byte_feature_file.write(file+\",\")\n",
    "    if(file.endswith(\"txt\")):\n",
    "        with open(root_byte + '/' + file,\"r\") as byte_flie:\n",
    "            for lines in byte_flie:\n",
    "                line=lines.rstrip().split(\" \")\n",
    "                for i in range(len(line)-1): # for i in range(len(line)-1):\n",
    "                    if('??' in line[i]+line[i+1]):#   if(line[i]+line[i+1] == '????')\n",
    "                        feature_matrix[k][65536]+=1 # feature_matrix[k][65536]+=1\n",
    "                    else:\n",
    "                        feature_matrix[k][int(line[i]+line[i+1],16)]+=1 # feature_matrix[k][int(line[i]+line[i+1],16)]+=1\n",
    "        byte_flie.close()\n",
    "    for i, row in enumerate(feature_matrix[k]):\n",
    "        if i!=len(feature_matrix[k])-1:\n",
    "            byte_feature_file.write(str(row)+\",\")\n",
    "        else:\n",
    "            byte_feature_file.write(str(row))\n",
    "    byte_feature_file.write(\"\\n\")\n",
    "    \n",
    "    k += 1\n",
    "\n",
    "byte_feature_file.close()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
